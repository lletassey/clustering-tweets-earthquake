{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Temporal Clustering of 2019 Teil Quake Tweets\n",
    "---\n",
    "\n",
    "In order to cluster the 2019 Teil Quake Tweets, we use ST-DBSCAN algorithm, a spatial-temporal clustering algorithm that is based on the DBSCAN algorithm augmented with a temporal parameter.\n",
    "\n",
    "We have used the following implementation [ST-DBSCAN](https://github.com/eren-ck/st_dbscan).\n",
    "> **Keywords** Data mining, Cluster analysis, Spatial–temporal data, Cluster visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "from cycler import cycler\n",
    "import matplotlib.pyplot as plt\n",
    "from st_dbscan import ST_DBSCAN\n",
    "from prettytable import PrettyTable\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Polygon\n",
    "from py_module.utils import create_hulls, plot_space_time_cube, plot_hulls\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set numpy print options\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Set up matplotlib rcParams (runtime configuration) for plot color\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "\n",
    "# Plotting pretty figures and avoid blurry images\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Use interactive matplotlib\n",
    "%matplotlib widget\n",
    "\n",
    "# Enable multiple cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling\n",
    "\n",
    "The preprocessing is done in the following steps:\n",
    "- Read tweets from hdf file ;\n",
    "- Remove tweets with no location information ;\n",
    "- Constrain tweets to France area ;\n",
    "- Order tweets by datetime ;\n",
    "- Calculate cumulative time ;\n",
    "- Create a new index ;\n",
    "- Remove columns that are not needed ;\n",
    "- Reproject coordinates to Lambert 93 ;\n",
    "- Calculate projected coordinates ;\n",
    "- Store x_m and y_m in separate numpy array for plotting ;\n",
    "- Store cumulative_time_sec, x_m and y_m in a seperate numpy array for clustering ;\n",
    "- Calculate pairwise distances ;\n",
    "- Reorder columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tweets from HDF5 file\n",
    "data_dir = Path(\"../data\")\n",
    "tweets = pd.read_hdf(data_dir / \"tweets.h5\", \"tweets\")\n",
    "\n",
    "# Remove tweets with no coordinates\n",
    "tweets = tweets[tweets[\"longitude\"].notnull()]\n",
    "\n",
    "# Constrain the area to France\n",
    "tweets = tweets.loc[\n",
    "        (tweets.latitude > 42.33278)\n",
    "    &   (tweets.latitude < 51.08917)\n",
    "    &   (tweets.longitude > -4.795556)\n",
    "    &   (tweets.longitude < +8.230556)\n",
    "]\n",
    "\n",
    "# Order tweets by datetime\n",
    "tweets['createdAt'] = pd.to_datetime(tweets['createdAt'])\n",
    "tweets.sort_values(by=\"createdAt\", inplace=True)\n",
    "\n",
    "# Convert datetime (in ns) to seconds, // (floor division) 10 ** 9 to get seconds\n",
    "tweets['cumulative_time_sec'] = (tweets['createdAt'].values.astype(np.int64) // 10 ** 9)\n",
    "# Convert ts in tweets to cumulative seconds\n",
    "tweets['cumulative_time_sec'] -= tweets['cumulative_time_sec'].min()\n",
    "\n",
    "# Set id as index\n",
    "tweets[\"id\"] = range(1, len(tweets) + 1)\n",
    "tweets.set_index('id', inplace=True, verify_integrity=False)\n",
    "\n",
    "# Remove columns that are not needed\n",
    "try:\n",
    "    tweets.drop(columns=['tweetId', 'mention', 'start', 'end', 'altitude', 'osm', 'geonames', 'entity', 'wikidata'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reproject tweets to Lam-93\n",
    "tweets = gpd.GeoDataFrame(tweets, geometry=gpd.points_from_xy(tweets.longitude, tweets.latitude), crs=\"EPSG:4326\")\n",
    "tweets = tweets.to_crs(\"EPSG:2154\")\n",
    "\n",
    "# Calculate projected coordinates\n",
    "tweets['x_m'] = tweets.geometry.x.round(3)\n",
    "tweets['y_m'] = tweets.geometry.y.round(3)\n",
    "\n",
    "# Will be used for plotting (we can't plot normalized coordinates)\n",
    "COORDS = tweets.loc[:, ['x_m','y_m']].values\n",
    "\n",
    "# Calculate all pairwise terrain distances\n",
    "dist = np.linalg.norm(COORDS[:, np.newaxis, :] - COORDS[np.newaxis, :, :], axis=-1)\n",
    "dist.sort(axis=1)\n",
    "\n",
    "# Scale coordinates to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(tweets[['x_m', 'y_m']])\n",
    "\n",
    "# ts, x_m_norm, y_m_norm\n",
    "data = np.hstack((tweets['cumulative_time_sec'].values.reshape(-1, 1), scaled))\n",
    "\n",
    "# Calculate all pairwise normalized distances\n",
    "dist_norm = np.linalg.norm(data[:, np.newaxis, 1:] - data[np.newaxis, :, 1:], axis=-1)\n",
    "dist_norm.sort(axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "tweets = tweets[['text', 'x_m', 'y_m', 'createdAt', 'cumulative_time_sec', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()\n",
    "tweets.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial temporal clustering of Quake Tweets\n",
    "#### Clustering with ST-DBSCAN\n",
    "\n",
    "We'll launch clustering with parameters `eps1 = 90 km`, `eps2 = 10 min` et `min_samples = 22 tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST-DBSCAN Clustering\n",
    "# Parameters\n",
    "eps1 = 0.1 # Distance threshold\n",
    "eps2 = 60*10 # Time threshold\n",
    "min_samples = 22 # Minimum number of tweets in a cluster\n",
    "\n",
    "# Calculate terrain distance threshold\n",
    "eps1_terrain = (eps1 * dist[-1][-1]) / dist_norm[-1][-1]\n",
    "\n",
    "# Run ST-DBSCAN\n",
    "start = time.time()\n",
    "st_dbscan = ST_DBSCAN(eps1 = eps1, eps2 = eps2, min_samples = min_samples, metric = 'euclidean').fit(data)\n",
    "end = time.time()\n",
    "\n",
    "# Label tweets with cluster id\n",
    "tweets['cluster'] = st_dbscan.labels\n",
    "tweets.set_index(pd.MultiIndex.from_tuples(list(zip(tweets['cluster'], range(1, len(tweets) + 1))), names=[\"cluster\", \"tweet_id\"]), inplace=True)\n",
    "tweets.sort_index(inplace=True)\n",
    "tweets.drop(columns=['cluster'], inplace=True)\n",
    "tweets = tweets[['text', 'x_m', 'y_m', 'createdAt', 'cumulative_time_sec', 'geometry']]\n",
    "\n",
    "# Print results\n",
    "\n",
    "result = [\n",
    "    ['eps1 (m)', 'eps2 (min)', 'MinPts', 'Time (s)', '# of clusters'],\n",
    "    [f'{int(eps1_terrain):,}', int(eps2 / 60), min_samples, round(end - start, 2), len(set(st_dbscan.labels))]\n",
    "]\n",
    "\n",
    "table = PrettyTable(result[0])\n",
    "table.add_row(result[1])\n",
    "print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convex hulls as Quake feeling areas\n",
    "\n",
    "In order to assess the extent of the quake, we'll create a convex hull for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hulls\n",
    "hulls = create_hulls(tweets, st_dbscan, COORDS)\n",
    "\n",
    "# Remove tweets with no hull\n",
    "hull_tweet_set = set(hulls.index)\n",
    "no_hull_tweet_set = set(tweets.index.get_level_values(0)) - set(hulls.index) - {-1}\n",
    "tweets = tweets[~tweets.index.get_level_values(0).isin(no_hull_tweet_set)]\n",
    "\n",
    "# Reset hulls index\n",
    "hulls.index = range(0, len(hulls))\n",
    "hulls.index.name = \"cluster\"\n",
    "\n",
    "# Reset tweets index\n",
    "tweets.reset_index('cluster', inplace=True)\n",
    "tweets.loc[tweets['cluster'] != -1, 'cluster'] = tweets.loc[tweets['cluster'] != -1, 'cluster'].astype('category').cat.codes\n",
    "\n",
    "# Recreate multiindex\n",
    "tweets.set_index(pd.MultiIndex.from_tuples(list(zip(tweets['cluster'], range(1, len(tweets) + 1))), names=[\"cluster\", \"tweet_id\"]), inplace=True)\n",
    "tweets.drop(columns=['cluster'], inplace=True)\n",
    "\n",
    "result = [\n",
    "    [\n",
    "        'Clusters with hulls',\n",
    "        'Clusters with no hulls'\n",
    "    ],\n",
    "    [\n",
    "        hull_tweet_set,\n",
    "        no_hull_tweet_set\n",
    "    ]\n",
    "]\n",
    "\n",
    "table = PrettyTable(result[0])\n",
    "table.add_row(result[1])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hulls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "#### Space time cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot space time cube\n",
    "plot_space_time_cube(tweets=tweets, eps1_terrain=eps1_terrain, eps2=eps2)\n",
    "plot_hulls(tweets=tweets, hulls=hulls, eps1_terrain=eps1_terrain, eps2=eps2, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tweets to geojson\n",
    "tweets.to_file(f'../data/tweets_stdbscan_eps1_{eps1_terrain / 1_000:.0f}_km_eps2_{int(eps2 / 60)}_min.geojson', driver='GeoJSON')\n",
    "\n",
    "# Export hulls to geojson\n",
    "hulls.to_file(f'../data/hulls_stdbscan_eps1_{eps1_terrain / 1_000:.0f}_km_eps2_{int(eps2 / 60)}_min.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Made with ♥ by Léa, Romain and Salaheddine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f2718f0011a7e62961ce8cd20a40265eb4dd1214a4e3e8c28ddc4905f6ec99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

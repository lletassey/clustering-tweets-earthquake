{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "data_dir = os.path.realpath(\"../data\")\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(os.path.join(data_dir, \"tweets.h5\")):\n",
    "    # Read tweets from htweets5\n",
    "    tweets = pd.read_hdf(os.path.join(data_dir, \"tweets.h5\"), \"tweets\")\n",
    "else:\n",
    "    # Convert tweets to htweets5\n",
    "    # ? hdf manages large data well\n",
    "    tweets = pd.read_csv(os.path.join(data_dir, \"tweets.csv\"))\n",
    "    tweets.to_hdf(os.path.join(data_dir, \"tweets.h5\"), \"tweets\")\n",
    "\n",
    "# Remove duplicates\n",
    "tweets.drop_duplicates(subset=\"wikidata\", inplace=True)\n",
    "\n",
    "# Remove null longitude and latitude values\n",
    "tweets = tweets[tweets[\"longitude\"].notnull()]\n",
    "\n",
    "# Constrain the area to France\n",
    "tweets = tweets.loc[\n",
    "    (tweets.latitude > 42.33278)\n",
    "    & (tweets.latitude < 51.08917)\n",
    "    & (tweets.longitude > -4.795556)\n",
    "    & (tweets.longitude < 8.230556)\n",
    "]\n",
    "\n",
    "# Set preprocessor options\n",
    "p.set_options(p.OPT.URL)\n",
    "\n",
    "# Remove URLs from the tweets\n",
    "tweets['text'] = tweets['text'].apply(lambda x: p.clean(x))\n",
    "\n",
    "# Set index to id for easy matching\n",
    "tweets.set_index('id', inplace=True, verify_integrity=False)\n",
    "\n",
    "# Convert lat and long to numpy array\n",
    "COORDS = tweets[['longitude', 'latitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DBSCAN parameters\n",
    "# ? Higher minsamples or lower eps indicate higher density necessary to form a cluster\n",
    "# ? On noisy and large datasets, higher min_samples is better\n",
    "EPS = 0.8\n",
    "MIN_SAMPLES = 5\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Create DBSCAN model\n",
    "# ? Noise is labeled -1\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit(COORDS)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: {} seconds\".format(end - start))\n",
    "\n",
    "# Print number of clusters\n",
    "print(\"Number of clusters: {}\".format(len(set(db.labels_))))\n",
    "\n",
    "# Store labels in tweets\n",
    "tweets['cluster'] = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Read France geojson\n",
    "france = gpd.read_file(filename='../data/france.geojson')\n",
    "\n",
    "# Convert tweets to geodataframe\n",
    "tweets_gdf = gpd.GeoDataFrame(\n",
    "    tweets,\n",
    "    geometry=gpd.points_from_xy(tweets.longitude, tweets.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read francegeojason file using geopandas\n",
    "france = gpd.read_file(filename='../data/france.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweets to geodataframe\n",
    "tweets_gdf = gpd.GeoDataFrame(\n",
    "    tweets,\n",
    "    geometry=gpd.points_from_xy(tweets.longitude, tweets.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot base map\n",
    "base = france.plot(color='#F2E7DC', figsize=(7, 7), linewidth=0)\n",
    "base.set_facecolor('#818274')\n",
    "\n",
    "# Plot tweets with cluster labels with different markers\n",
    "for cluster in tweets_gdf.cluster.unique():\n",
    "    tweets_gdf[tweets_gdf.cluster == cluster].plot(\n",
    "        ax=base,\n",
    "        markersize=10,\n",
    "        label='NOISE' if cluster == -1 else 'Cluster {}'.format(cluster),\n",
    "        alpha=0.7,\n",
    "        # linewidth=0.6,\n",
    "        marker='x' if cluster != -1 else '+',\n",
    "        color='black' if cluster == -1 else 'C{}'.format(cluster + 3),\n",
    "        linewidth=0.6,\n",
    "    )\n",
    "\n",
    "# Set title\n",
    "base.set_title(\"DBSCAN Clustering of Tweets in France\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure\n",
    "base.get_figure().savefig('../images/dbscan.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f2718f0011a7e62961ce8cd20a40265eb4dd1214a4e3e8c28ddc4905f6ec99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

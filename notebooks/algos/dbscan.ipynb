{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering of Quake Tweets\n",
    "\n",
    "We implement the DBSCAN algorithm in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import preprocessor as p\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "data_dir = os.path.realpath(\"../data\")\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(os.path.join(data_dir, \"tweets.h5\")):\n",
    "    # Read tweets from htweets5\n",
    "    tweets = pd.read_hdf(os.path.join(data_dir, \"tweets.h5\"), \"tweets\")\n",
    "else:\n",
    "    # Convert tweets to htweets5\n",
    "    # ? hdf manages large data well\n",
    "    tweets = pd.read_csv(os.path.join(data_dir, \"tweets.csv\"))\n",
    "    tweets.to_hdf(os.path.join(data_dir, \"tweets.h5\"), \"tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "# Test if wikidata column exists\n",
    "if \"wikidata\" in tweets.columns:\n",
    "    tweets.drop_duplicates(subset=\"wikidata\", inplace=True)\n",
    "\n",
    "# Remove null longitude and latitude values\n",
    "tweets = tweets[tweets[\"longitude\"].notnull()]\n",
    "\n",
    "# Constrain the area to France\n",
    "tweets = tweets.loc[\n",
    "    (tweets.latitude > 42.33278)\n",
    "    & (tweets.latitude < 51.08917)\n",
    "    & (tweets.longitude > -4.795556)\n",
    "    & (tweets.longitude < 8.230556)\n",
    "]\n",
    "\n",
    "# Set preprocessor options\n",
    "p.set_options(p.OPT.URL)\n",
    "\n",
    "# Remove URLs from the tweets\n",
    "tweets['text'] = tweets['text'].apply(lambda x: p.clean(x))\n",
    "\n",
    "# Order tweets by createdAt\n",
    "tweets.sort_values(by=\"createdAt\", inplace=True)\n",
    "\n",
    "# Set id as index\n",
    "tweets[\"id\"] = range(1, len(tweets) + 1)\n",
    "tweets.set_index('id', inplace=True, verify_integrity=False)\n",
    "\n",
    "# Remove columns that are not needed\n",
    "if \"wikidata\" in tweets.columns:\n",
    "    tweets.drop(columns=['tweetId', 'mention', 'start', 'end', 'wikidata', 'altitude', 'osm', 'geonames'], inplace=True)\n",
    "\n",
    "# Convert tweets to geodataframe with crs\n",
    "tweets = gpd.GeoDataFrame(tweets, geometry=gpd.points_from_xy(tweets.longitude, tweets.latitude), crs=\"EPSG:4326\")\n",
    "\n",
    "# Convert crs to lambert93\n",
    "tweets = tweets.to_crs(\"EPSG:2154\")\n",
    "\n",
    "# Calculate x and y coordinates and round them\n",
    "tweets['x_m'] = tweets.geometry.x.round(3)\n",
    "tweets['y_m'] = tweets.geometry.y.round(3)\n",
    "\n",
    "# Move x_m and y_m after latitude and longitude\n",
    "tweets = tweets[['text', 'entity', 'x_m', 'y_m', 'createdAt', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lat and long to numpy array\n",
    "COORDS = tweets[['x_m', 'y_m']].values\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "# ? Higher minsamples or lower eps indicate higher density necessary to form a cluster\n",
    "# ? On noisy and large datasets, higher min_samples is better\n",
    "EPS = 20_000 # 20km\n",
    "MIN_SAMPLES = 5\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Create DBSCAN model\n",
    "# ? Noise is labeled -1\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit(COORDS)\n",
    "\n",
    "# End timer\n",
    "end = time.time()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: {} seconds\".format(end - start))\n",
    "\n",
    "# Print number of clusters\n",
    "print(\"Number of clusters: {}\".format(len(set(db.labels_))))\n",
    "\n",
    "# Store labels in tweets\n",
    "tweets['cluster'] = db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create convex hulls for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hulls geodataframe\n",
    "hulls = gpd.GeoDataFrame(columns=['cluster', 'num_tweets', 'area_km2', 'geometry'])\n",
    "\n",
    "for cluster in set(db.labels_):\n",
    "    if cluster != -1:\n",
    "        # Get the coordinates of the points in the cluster\n",
    "        points = COORDS[db.labels_ == cluster]\n",
    "        # Create a convex hull\n",
    "        hull = ConvexHull(points)\n",
    "        # Append the cluster and hull to the geodataframe using concat instead of append\n",
    "        hulls = pd.concat([hulls, gpd.GeoDataFrame({\n",
    "            'cluster': [cluster],\n",
    "            'num_tweets': [len(points)],\n",
    "            'geometry': [Polygon(points[hull.vertices])]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "hulls.set_crs(epsg=2154, inplace=True)\n",
    "\n",
    "# Calculate the area of the hulls\n",
    "hulls['area_km2'] = round(hulls['geometry'].area / 1_000_000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read France geojson\n",
    "france = gpd.read_file(filename='../data/france.geojson')\n",
    "\n",
    "# Convert crs to lambert93\n",
    "france = france.to_crs(\"EPSG:2154\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot base map\n",
    "base = france.plot(color='#F2E7DC', figsize=(7, 7), linewidth=0)\n",
    "base.set_facecolor('#818274')\n",
    "\n",
    "# Plot tweets with cluster labels with different markers\n",
    "for cluster in tweets.cluster.unique():\n",
    "    tweets[tweets.cluster == cluster].plot(\n",
    "        ax=base,\n",
    "        markersize=10,\n",
    "        label='NOISE' if cluster == -1 else 'Cluster {}'.format(cluster),\n",
    "        alpha=0.7,\n",
    "        # linewidth=0.6,\n",
    "        marker='x' if cluster != -1 else '+',\n",
    "        color='black' if cluster == -1 else 'C{}'.format(cluster + 3),\n",
    "        linewidth=0.6,\n",
    "    )\n",
    "    \n",
    "for cluster in hulls.cluster:\n",
    "    hulls[hulls.cluster == cluster].plot(\n",
    "        ax=base,\n",
    "        alpha=0.3,\n",
    "        color='C{}'.format(cluster + 3),\n",
    "        edgecolor='black',\n",
    "        linewidth=0.6,\n",
    "    )\n",
    "\n",
    "# Set title\n",
    "base.set_title(\"DBSCAN Clustering of Tweets in France\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure\n",
    "base.get_figure().savefig('../images/dbscan.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f2718f0011a7e62961ce8cd20a40265eb4dd1214a4e3e8c28ddc4905f6ec99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
